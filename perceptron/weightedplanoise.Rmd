---
title: "Weighted perceptron with noise"
output: github_document
---

In this document, I will use Perceptron Learning Algorithm (PLA) to classify some non linearly separable 2D data. These data are generated by adding some noises to the original linearly separable data. To use PLA for non linearly separable data, some adjustments needed to be made to the algorithm. More specifically, instead of repeating the algorithm until no more points are misclassified (which is unachievable with non linearly separable data), we will have to specify the number of iterations to run and keep the set of weights that achieves the best performance as our final hypothesis. This algorithm that keeps the best solution while looking for others is called pocket algorithm. 

Besides introducing noises, I will also attribute different weights to the two classes of data in order to decrease one type of classification error (often at the expenses of increasing the other type). Having different weights is very useful in practice when one type of error is more undesirable than the other.

## Data
I start by generating a target function, 100 training points and 1000 testing points. The points are labeled +1 or -1 depending on whether they are above or below the line. To make the data non linearly separable, I introduce 5% of error to both the training and the testing points.
```{r}
## Find a separating line
PX1 <-runif(2,min=-1,max=1)
PX2 <-runif(2,min=-1,max=1)
slope <- diff(PX2)/diff(PX2)
intercept <- PX2[1]-slope*PX1[1]

## Training points
ntrain <- 100
X1 <- runif(ntrain,min=-1,max=1)
X2 <- runif(ntrain,min=-1,max=1)
X <- cbind(rep(1,ntrain),X1,X2)
Y <- ifelse(X2>intercept+slope*X1,+1,-1)
## Add 5% errors
err <- sample(ntrain,ntrain*0.05)
Y[err] <- -Y[err] 

## Testing points
ntest<- 1000
X1_test <- runif(ntest,min=-1,max=1)
X2_test <- runif(ntest,min=-1,max=1)
X_test <- cbind(rep(1,ntest),X1_test,X2_test)
Y_test <- ifelse(X2_test>intercept+slope*X1_test,+1,-1)
## Add 5% errors
err <- sample(ntest,ntest*0.05)
Y_test[err] <- -Y_test[err] 
```
## Training
### Unweighted PLA
The pocket algorithm that adapts PLA for non lineary separable data compares the in-sample error of the set of weights obtained from the current iteration and that from the previous iteration and keeps the one with the lower in-sample error.
```{r}
## Initialize
w_uw<- c(0,0,0)
Y_hat <- X %*% w_uw
count <- 1
change <- 1
maxit <- 1000
## PLA
Einvec <- NULL
while (count < maxit)
{
  miss_class <- which(sign(Y_hat)!=sign(Y))
  n <- ifelse(length(miss_class)>1,sample(miss_class,1),miss_class)
  w_temp <- w_uw+(Y[n]%*% X[n,])
  Y_hat <- X %*% t(w_temp)
  Ein <- length(which(sign(Y_hat)!=Y))
  Einvec <- rbind(Einvec,Ein)
  if (count >1)
  {
    if (Einvec[count]<Einvec[count-1]){change <- 1}else{change <- 0}
  }
  if (change ==1)
  {
    w_uw <- w_temp
  }
  Y_hat <- X %*% t(w_uw)
  count <- count+1
}

```
### Weighted PLA
The goal of assigning different weights to the two classes of data is to emphasize the importance of one class and decrease the chance that the model commits errors on this class. This is useful, for example, in a disease diagnosis setting where we would like to put more emphasis on the positive cases (infected) then on the negative cases (not infected) when training our model. Here, I assign 9 times more weights to positive points (those labeled $+1$) than negative points(those labeled $-1$). Intuitively, this can be done by increasing the number of positive points in the data through copying and pasting. However, to avoid the consumption of unnecessary computational resources, an alternative way to achieve the same effect is to increase the chance that the algorithm stumbles across positive points when choosing a point to adjust its weights and the error associated with misclassifying positive points. 
```{r}
## Initialize
weight <- 9
w_w<- c(0,0,0)
Y_hat <- X %*% w_w
count <- 1
change <- 1
## PLA
Einvec <- NULL
while (count < maxit)
{
  miss_class <- which(sign(Y_hat)!=sign(Y))
  len1 <-length(which(Y[miss_class] ==1))
  lenm1 <-length(which(Y[miss_class] ==-1))
  prob <-ifelse(Y[miss_class]==1,(len1*weight/(lenm1+len1*weight))/len1, (lenm1/(lenm1+len1*weight))/lenm1)
  n <- ifelse(length(miss_class)>1,sample(miss_class,1,prob=c(prob)),miss_class)
  w_temp <- w_w+(Y[n]%*% X[n,])
  Y_hat <- X %*% t(w_temp)
  Ein <- length(which(sign(Y_hat)!=Y&Y==1))*weight+length(which(sign(Y_hat)!=Y&Y==-1))
  Einvec <- rbind(Einvec,Ein)
  if (count >1)
  {
    if (Einvec[count]<Einvec[count-1]){change <- 1}else{change <- 0}
  }
  if (change ==1)
  {
    w_w <- w_temp
  }
  Y_hat <- X %*% t(w_w)
  count <- count+1
}

```

## Testing
Let's compare the performance of the unweighted and the weighted PLA. In most of the cases (as can be verified by repeating the above algorithm multiple times), assigning more weights to positive points decreases the number of positive points being classified as negative. This is exactly what we want.
```{r,warning=F,message=F}
Y_hat_test_uw <- X_test %*% t(w_uw)
Eout_unw <- length(which(sign(Y_hat_test_uw)!=Y_test))/length(Y_test) 
Y_hat_test_w <- X_test %*% t(w_w)
Eout_w <- length(which(sign(Y_hat_test_w)!=Y_test))/length(Y_test) 

table(actual=Y_test,predicted_unweighted=sign(Y_hat_test_uw))

table(actual=Y_test,predicted_weighted=sign(Y_hat_test_w))
```

However, this comes often at the expenses of increasing the overall out-of-sample error.
```{r}
Eout_unw # Out-of-sample error of the unweighted PLA
Eout_w # Out-of-sample error of the weighted PLA

```

The target function and the final hypothesis of the unweighted and the weighted PLA are presented in the graph below along with the test points. It can be observed that the final hypothesis of the weighted PLA falls lower to the negative zone thereby classifies more points as positive. Considering that we told the algorithm to avoid classifying positive points as negative, this is an expected outcome.

```{r,message=FALSE,warning=FALSE}
library(ggplot2)
int_w <- -w_w[,1]/w_w[,3]
slope_w <- -w_w[,2]/w_w[,3]
int_uw <- -w_uw[,1]/w_uw[,3]
slope_uw <- -w_uw[,2]/w_uw[,3]

X_test <- as.data.frame(X_test)

ggplot()+
geom_point(data=X_test,aes(X_test$X1_test,X_test$X2_test,color=as.factor(Y_test)))+
geom_abline(aes(intercept = intercept,slope=slope,linetype="Target"))+
geom_abline(aes(intercept = int_w,slope=slope_w,linetype="Weighted"))+
geom_abline(aes(intercept = int_uw,slope=slope_uw,linetype="Unweighted"))+
labs(x="X1",y="X2",color="",title="")+
scale_linetype_manual(name ="",values=c(1,2,3))



```

