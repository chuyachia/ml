---
title: "Perceptron"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



This document demonstrates the application of the Perceptron Learning Algorithm (PLA) to some linearly separable 2D data for the purpose of classification.

## Preparing the data

First, we randomly generate 100 points with coordinates x1, x2 and an intercept x0 as our training data:

```{r}
X1 <- runif(100,min=-1,max=1)
X2 <- runif(100,min=-1,max=1)
X0 <- rep(1,100)
X <- cbind(X0,X1,X2)
```

We then generate a random line on the plane as the target function. Points above the line are labeled $y= +1$ whereas those below are labeled $y= -1$.

```{r}
PX1 <-runif(2,min=-1,max=1)
PX2 <-runif(2,min=-1,max=1)
slope <- diff(PX2)/diff(PX2)
intercept <- PX2[1]-slope*PX1[1]
Y <- ifelse(X2>intercept+slope*X1,+1,-1)
X.plot <- as.data.frame(X) # dataframe for plot use
```

This is what the data points and the target function look like on a 2D plane :

```{r}
library(ggplot2)
```

```{r}
ggplot(X.plot,aes(X.plot$X1,X.plot$X2))+
    geom_point(aes(color=as.factor(Y)),show.legend = T)+
    geom_abline(intercept = intercept,slope=slope)+
    labs(x="X1",y="X2",color="")
```

## Hypothesis 
The hypothesis set of perceptron assign +1 or -1 to y according to the sign of the dot production of the vector $x$ (containing x0, x1, x2) and a weight vector $w$ (containing w0, w1, w2).

$h(x) = sign(w^tx)$

## PLA
The goal of PLA is to find a set of optimal weights such that $sign(w^tx_n)$ would correctly predict $yn$ for all point $n$ in our data. 

To do so, the algorithm randomly picks a misclassified point $n$ in each round and updates the weight vector by adding the product of the scalar $y_n$ and the vector $x_n$ to it. 

$w_{t+1} = w_t+ y_nx_n$  

By doing so, PLA rotate the weight vector w towards the misclassified point since $w_{t+1}x_n > w_tx_n$.

These steps are repeated until all points are correctly classified. 

```{r}
## Initialize
w <- c(0,0,0)
count <- 0 # to count the number of rounds required
wdf <- NULL # df to collect the resulting weights of each round for plot use
Y_hat <- X %*% w
wdf <- rbind(wdf,w)
## Perceptron learning algorithm
while (any(sign(Y_hat)!=sign(Y)))
{
  miss_class <- which(sign(Y_hat)!=sign(Y))
  ifelse(length(miss_class)>1,n <- sample(miss_class,1),n <- miss_class)
  w <- w+(Y[n]%*% X[n,])
  wdf <- rbind(wdf,w)
  Y_hat <- X %*% t(w)
  count <- count+1
}

```

The final weights of the trained model are shown below by the dashed line which is close to the original target function :
```{r}
drawplot <- function(n,name)
{
  if (!n %in% seq(1,count+1))
  {
    warning("Out of bounds")
  }
  else
  {
  int_w <- -wdf[n,1]/wdf[n,3]
  slope_w <- -wdf[n,2]/wdf[n,3]
  a <- ggplot(X.plot,aes(X.plot$X1,X.plot$X2))+
    geom_point(aes(color=as.factor(Y)),show.legend = F)+
    geom_abline(intercept = intercept,slope=slope)+
    geom_abline(intercept = int_w,slope=slope_w,linetype="dashed")+
    labs(x="X1",y="X2",color="",title=name)
  }
}
pf <- drawplot(count+1,"Final round")
pf
```

We can also see the resulting weights of the last four rounds :

```{r}
library(gridExtra)
p1 <- drawplot(count-2,sprintf("Round %i",count-3))
p2 <- drawplot(count-1,sprintf("Round %i",count-2))
p3 <- drawplot(count,sprintf("Round %i",count-1))
grid.arrange(p1, p2,p3,pf,ncol=2)

```
