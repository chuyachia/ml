---
title: "Logistic regression"
output: github_document
---

In this document, I will use logistic regression with stochastic gradient descent as optimization algorithm to classify different species of iris. 

## Preparing the data
The data used here to train the model comes from the R default iris data set which includes the length and the width of the petals and the sepals of three different species of iris. Here, I will use only two of the four features of iris- sepal length and sepal width. 

```{r}
library(dplyr)
data(iris)
D <- iris %>% select(-c(Petal.Length,Petal.Width))
```


A quick summary table shows that the three species of iris indeed tend to differ in the length and the width of their sepal.
```{r}
D %>% group_by(Species) %>% summarize(meanSepal.L= mean(Sepal.Length),meanSepal.W= mean(Sepal.Width))
```

Before going on with logistic regression, I first normalize the two features and add an intercept to each observation. Since I'm doing binary classification, I'm only going classify the observations into Setosas and non-Setosas (Versicolors and Virginicas). The outputs (the species) of iris are coded 1 if they belong to Setosa and -1 otherwise.

```{r}
X <- as.matrix(D  %>% mutate(Sepal.L.Norm =
                               Sepal.Length-mean(Sepal.Length),Sepal.W.Norm =
                               Sepal.Width-mean(Sepal.Width),intercept=1)%>%
                 select(-c(Species,Sepal.Length,Sepal.Width))) 
Y <- ifelse(D$Species=="setosa",1,-1)

```

## Logistic regression
Essentially, what logistic regression does is passing the signal, $s$, which is the result of the dot product between a vector composed of the features of each observation $X_{n}$ and a vector of weights w, through a logistic function, theta, defined as : $theta(s) = e^s/(1+e^s)$. 
The output of the logistic function is bounded between 0 and 1.

The algorithm used to find the optimal weights vector is gradient descent, which consists of updating the weights vector to the opposite direction of the gradient of the cost function. The cost function used here is the cross-entropy error function. Here, I'm using the stochastic version of gradient descent because, instead of computing the gradient with regards to all observations and taking the average to update the weights vector, I randomly choose one observation at a time and update directly the weights vector by the gradient with regards to this observation.

Unlike with perceptron on linearly separable data where the algorithm stops automatically when there is no more misclassified data, here, a stop condition has to be defined for the algorithm. I will tell the algorithm to stop when the variation (Euclidean distance) between the resulting weights vector of the present round and the weights vector from the last round is smaller than a threshold. 

In the following codes, I define three functions to calculate the cross-entropy error, its gradient and the Euclidean distance between two weights vectors.

```{r}
## Function
# Cross-entropy error
E <- function(x,y,z) 
{
  log(1+exp(-y%*%z%*%x))
}
# Gradient 
grad <- function(x,y,z)
{
  (-y%*%x)/c(exp(y%*%z%*%x)+1)
}
# Function to calculate Euclidean distance
norm_vec <- function(x) sqrt(sum(x^2))

```

To start the algorithm, I initialize the weights vector to all zeros. I also set the parameters to desired values. The two parameters that have to set are the rate of gradient descent and the threshold of variation of the weights vector below which the algorithm should stop.

```{r}
## Parameter
rate <- 0.01
thres <- 0.01
## Initialization
w <- rep(0,ncol(X))
w.vec <- NULL
w.vec <- rbind(w.vec,w)
e <- NULL
e.vec <- NULL
epoch <- 1
w.var <-1 

while (w.var>=thres)
{
  for (i in 1 : nrow(X))
  {
    e[i] <- E(X[i,],Y[i],w.vec[epoch,])
  }
  e.vec <- rbind(e.vec,mean(e))
  
  seq <-sample(c(1:nrow(X)),nrow(X),replace=F)
  for (i in 1:100) 
  {
    gradient <- grad(X[seq[i],],Y[seq[i]],w)
    w <- w -rate%*%gradient
  }
  w.vec <- rbind(w.vec,w)
  epoch = epoch+1
  w.var <-norm_vec(w.vec[epoch-1,]-w.vec[epoch,])
  
}
```


## Result
The following plot shows that the average cross-entropy error of the training data decreases over rounds.

```{r}
plot(e.vec,type="l",xlab="Round",ylab="Average cross-entropy error")
```

Using the final weights vector, I compute the signal for every observation and decides that a given observation is a Setosa if the logistic function on its signal results in a value that is greater than 0.5. In probabilistic terms, this means that the probability that the given observation is Setosa is greater than 0.5. By doing so, I successfully distinguished all the Setosas from the Non-Setosas in the training data. This shows that the algorithm is working correctly. 

```{r}
Y.sig <- exp(X %*% t(w))/(1+exp(X%*%t(w)))
Y.predict <- ifelse(Y.sig >0.5,1,-1)
length(which(Y.predict != Y)) ## zero misclassified observation
```

